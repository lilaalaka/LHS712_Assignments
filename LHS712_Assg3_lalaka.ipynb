{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3d233b-0c1c-4904-bfa0-cf24b2db9e3a",
   "metadata": {},
   "source": [
    "## LHS 712 Assg 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032852e1-384c-4809-82bf-35ead9b1f0ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **CRF Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8578d9e-3a3b-48b5-a1bb-27069098861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "\n",
    "def read_file(f):\n",
    "    # Read all the lines in\n",
    "    data = open(f,'r').readlines()[1:]\n",
    "    # Split on tab and save row_id\n",
    "    row_id = [i.split('\\t')[0].strip() for i in data]\n",
    "    # Split on tab then split words by space\n",
    "    data = [i.split('\\t')[1].strip().split(' ') for i in data]\n",
    "    return row_id,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "522157c7-51ef-4b95-9e21-e8f106a08415",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'REVIEW_TEXT.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zs/vb0jk42159g_18xt9v8_mbqc0000gn/T/ipykernel_42921/931277333.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrow_id_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'REVIEW_TEXT.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrow_id_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'REVIEW_LABELSEQ.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# row_id_text should match exactly with row_id_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# len(texts) and len(tags) should match exactly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zs/vb0jk42159g_18xt9v8_mbqc0000gn/T/ipykernel_42921/4283750455.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Read all the lines in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Split on tab and save row_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrow_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'REVIEW_TEXT.txt'"
     ]
    }
   ],
   "source": [
    "row_id_text, texts = read_file('REVIEW_TEXT.txt')\n",
    "row_id_tags, tags = read_file('REVIEW_LABELSEQ.txt')\n",
    "\n",
    "# row_id_text should match exactly with row_id_tags\n",
    "# len(texts) and len(tags) should match exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802ff784-623a-4a12-82f9-9e0cf2b12933",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'row_id_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zs/vb0jk42159g_18xt9v8_mbqc0000gn/T/ipykernel_42921/4183759673.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrow_id_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'row_id_text' is not defined"
     ]
    }
   ],
   "source": [
    "row_id_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d1c5835e-1d77-443b-960b-db4eb60b93ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features (add features for improvement)\n",
    "import string\n",
    "\n",
    "def word2features(word):\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.upper()': word.upper(),\n",
    "        'word[:1]': word[:1],\n",
    "        'word[:2]': word[:2],\n",
    "        'word[:3]': word[:3],\n",
    "        'word[:4]': word[:4],\n",
    "        'word[-1:]': word[-1:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-4:]': word[-4:],\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.ispunctuation()': (word in string.punctuation),\n",
    "        'len(word)': len(word)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# run word2features on every word in text\n",
    "def text2features(text):\n",
    "    return [word2features(i) for i in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c7c5bddb-e63f-41e4-82fc-63f30cba01ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [text2features(text) for text in texts]\n",
    "y = tags\n",
    "# x is now features and y is associated tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "20307efc-c716-483b-ad2b-095666d7170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Train / Validation Sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "007cf73a-5926-48a6-abb4-909a00d07ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-AE       0.67      0.62      0.65       827\n",
      "       B-SSI       0.58      0.59      0.58       150\n",
      "        I-AE       0.72      0.47      0.57      1513\n",
      "       I-SSI       0.28      0.13      0.18        82\n",
      "           O       0.91      0.96      0.94     11939\n",
      "\n",
      "    accuracy                           0.88     14511\n",
      "   macro avg       0.63      0.55      0.58     14511\n",
      "weighted avg       0.87      0.88      0.87     14511\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilaalaka/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "# Running the CRF model\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "crf = CRF(algorithm = 'ap')\n",
    "\n",
    "# Training and Prediction\n",
    "\n",
    "crf.fit(x_train, y_train)\n",
    "y_pred = crf.predict(x_validation)\n",
    "\n",
    "# Generating results\n",
    "\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "report = flat_classification_report(y_validation, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9df205-afb4-472f-be6d-158f39226d5e",
   "metadata": {},
   "source": [
    "# **LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5015bed3-3d99-4595-b276-203ac19ba397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(f):\n",
    "    data = open(f,'r').readlines()[1:]\n",
    "    row_id = [i.split('\\t')[0].strip() for i in data]\n",
    "    data = [i.split('\\t')[1].strip().split(' ') for i in data]\n",
    "    return row_id,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5f98ad2a-bd1c-4fb8-948a-ea75682ac83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id_text, texts = read_file('REVIEW_TEXT.txt')\n",
    "row_id_tags, tags = read_file('REVIEW_LABELSEQ.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "08978b2c-56ec-4867-b24d-44992cb1fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "texts_stemmed = []\n",
    "\n",
    "for text in texts:\n",
    "    def snow_stem(text):\n",
    "        stem = [snow_stemmer.stem(word) for word in text]\n",
    "        return stem\n",
    "    text = snow_stem(text)\n",
    "    texts_stemmed.append(text)\n",
    "\n",
    "texts = texts_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ee561e42-e3dd-4823-bd3f-7da9ceb7ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Find all the unique words from the text\n",
    "# Assign every word a number\n",
    "# Add special padding word with index 0\n",
    "unique_words = list(set([j for i in texts for j in i]))\n",
    "word2idx = {j:i+1 for i,j in enumerate(unique_words)}\n",
    "word2idx[\"PAD\"] = 0\n",
    "\n",
    "# Collection of tags with label to index and index to label mapping\n",
    "unique_tags = list(set([j for i in tags for j in i]))\n",
    "label2idx = {j:i for i,j in enumerate(unique_tags)}\n",
    "idx2label = {j:i for i,j in label2idx.items()}\n",
    "\n",
    "# pad sequences makes sequence lengths the same for NN model\n",
    "x = [[word2idx[j] for j in i] for i in texts]\n",
    "x = pad_sequences(maxlen = 25, sequences = x, padding = \"post\", value = word2idx[\"PAD\"])\n",
    "y = [[label2idx[j] for j in i] for i in tags]\n",
    "y = pad_sequences(maxlen = 25, sequences = y, padding = \"post\", value = label2idx[\"O\"])\n",
    "y = [to_categorical(i, num_classes = len(unique_tags)) for i in y]\n",
    "# tf_keras documentation reference\n",
    "# could use word embedding to convert word to vector instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8794ca38-6522-4080-8ef7-ae22015075b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_34 (Embedding)    (None, 25, 20)            173680    \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 25, 5)            105       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " bidirectional_34 (Bidirecti  (None, 25, 100)          22400     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 25, 5)             505       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 196,690\n",
      "Trainable params: 196,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Training and Validation Sets\n",
    "x_train, x_validation, y_train, y_validation  = train_test_split(x, y, test_size = 0.2)\n",
    "\n",
    "# Running LSTM Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Activation, Dropout, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word2idx.keys()),output_dim=20,input_length=25))\n",
    "model.add(TimeDistributed(Dense(len(label2idx.keys()))))\n",
    "model.add(Bidirectional(LSTM(units=50,return_sequences=True), merge_mode = 'concat'))\n",
    "model.add(Dense(len(label2idx.keys()), activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Reference Tensor Flow documentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8d8a1a23-527e-40a5-9665-df67af6c20ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "214/214 [==============================] - 6s 16ms/step - loss: 0.4542 - accuracy: 0.9044 - val_loss: 0.2773 - val_accuracy: 0.9172\n",
      "Epoch 2/8\n",
      "214/214 [==============================] - 3s 12ms/step - loss: 0.2511 - accuracy: 0.9116 - val_loss: 0.2027 - val_accuracy: 0.9265\n",
      "Epoch 3/8\n",
      "214/214 [==============================] - 3s 13ms/step - loss: 0.1964 - accuracy: 0.9287 - val_loss: 0.1815 - val_accuracy: 0.9343\n",
      "Epoch 4/8\n",
      "214/214 [==============================] - 3s 13ms/step - loss: 0.1590 - accuracy: 0.9435 - val_loss: 0.1771 - val_accuracy: 0.9411\n",
      "Epoch 5/8\n",
      "214/214 [==============================] - 3s 14ms/step - loss: 0.1301 - accuracy: 0.9566 - val_loss: 0.1632 - val_accuracy: 0.9472\n",
      "Epoch 6/8\n",
      "214/214 [==============================] - 3s 12ms/step - loss: 0.1057 - accuracy: 0.9655 - val_loss: 0.1734 - val_accuracy: 0.9483\n",
      "Epoch 7/8\n",
      "214/214 [==============================] - 3s 13ms/step - loss: 0.0890 - accuracy: 0.9701 - val_loss: 0.1588 - val_accuracy: 0.9524\n",
      "Epoch 8/8\n",
      "214/214 [==============================] - 3s 13ms/step - loss: 0.0728 - accuracy: 0.9757 - val_loss: 0.1830 - val_accuracy: 0.9541\n"
     ]
    }
   ],
   "source": [
    "# Training and Prediction\n",
    "\n",
    "import numpy as np\n",
    "history = model.fit(x_train,np.array(y_train),batch_size=16,epochs=8,validation_split=0.1)\n",
    "\n",
    "y_pred = model.predict(x_validation)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_validation = np.argmax(y_validation, -1)\n",
    "y_pred = [[idx2label[i] for i in row] for row in y_pred]\n",
    "y_validation = [[idx2label[i] for i in row] for row in y_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "dbd7244b-860a-4537-8924-cb019cf53327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-AE       0.80      0.53      0.64       695\n",
      "       B-SSI       0.65      0.58      0.61       161\n",
      "        I-AE       0.70      0.47      0.56      1329\n",
      "       I-SSI       0.14      0.02      0.03       108\n",
      "           O       0.95      0.99      0.97     21432\n",
      "\n",
      "    accuracy                           0.94     23725\n",
      "   macro avg       0.65      0.52      0.56     23725\n",
      "weighted avg       0.93      0.94      0.93     23725\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilaalaka/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "report = flat_classification_report(y_pred=y_pred, y_true=y_validation)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271c363-181f-4fe4-bf84-d757ddb1e7e7",
   "metadata": {},
   "source": [
    "## **Running LSTM on Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "23767311-a5e9-46a9-9226-64eb6fb56875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(f):\n",
    "    data = open(f,'r').readlines()[1:]\n",
    "    row_id = [i.split('\\t')[0].strip() for i in data]\n",
    "    data = [i.split('\\t')[1].strip().split(' ') for i in data]\n",
    "    return row_id,data\n",
    "\n",
    "row_id_text, texts = read_file('REVIEW_TEXT.txt')\n",
    "row_id_tags, tags = read_file('REVIEW_LABELSEQ.txt')\n",
    "row_id_text_test, texts_test = read_file('TEST_REVIEW_TEXT.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "334c5165-2c43-41ca-9e1b-aee5b55bdada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Find all the unique words from the text\n",
    "# Assign every word a number\n",
    "# Add special padding word with index 0\n",
    "unique_words = list(set([j for i in texts for j in i]))\n",
    "word2idx = {j:i+1 for i,j in enumerate(unique_words)}\n",
    "word2idx[\"PAD\"] = 0\n",
    "\n",
    "# Collection of tags with label to index and index to label mapping\n",
    "unique_tags = list(set([j for i in tags for j in i]))\n",
    "label2idx = {j:i for i,j in enumerate(unique_tags)}\n",
    "idx2label = {j:i for i,j in label2idx.items()}\n",
    "\n",
    "x = [[word2idx[j] for j in i] for i in texts]\n",
    "x = pad_sequences(maxlen = 25, sequences = x, padding = \"post\", value = word2idx[\"PAD\"])\n",
    "y = [[label2idx[j] for j in i] for i in tags]\n",
    "y = pad_sequences(maxlen = 25, sequences = y, padding = \"post\", value = label2idx[\"O\"])\n",
    "y = [to_categorical(i, num_classes = len(unique_tags)) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "528945b5-0fa6-4a86-9777-7c3abcb7f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_test = list(set([j for i in texts_test for j in i]))\n",
    "word2idx_test = {j:i+1 for i,j in enumerate(unique_words_test)}\n",
    "word2idx_test[\"PAD\"] = 0\n",
    "\n",
    "x_test = [[word2idx_test[j] for j in i] for i in texts_test]\n",
    "x_test = pad_sequences(maxlen = 25, sequences = x_test, padding = \"post\", value = word2idx_test[\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "41ee7a04-1065-4d4f-8890-a6b9656a2933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_40 (Embedding)    (None, 25, 20)            173680    \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 25, 5)            105       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " bidirectional_40 (Bidirecti  (None, 25, 100)          22400     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 25, 5)             505       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 196,690\n",
      "Trainable params: 196,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/8\n",
      "297/297 [==============================] - 7s 14ms/step - loss: 0.4108 - accuracy: 0.9050\n",
      "Epoch 2/8\n",
      "297/297 [==============================] - 4s 13ms/step - loss: 0.2314 - accuracy: 0.9179\n",
      "Epoch 3/8\n",
      "297/297 [==============================] - 4s 12ms/step - loss: 0.1725 - accuracy: 0.9390\n",
      "Epoch 4/8\n",
      "297/297 [==============================] - 4s 13ms/step - loss: 0.1295 - accuracy: 0.9560\n",
      "Epoch 5/8\n",
      "297/297 [==============================] - 4s 14ms/step - loss: 0.0986 - accuracy: 0.9666\n",
      "Epoch 6/8\n",
      "297/297 [==============================] - 4s 13ms/step - loss: 0.0762 - accuracy: 0.9742\n",
      "Epoch 7/8\n",
      "297/297 [==============================] - 4s 13ms/step - loss: 0.0599 - accuracy: 0.9795\n",
      "Epoch 8/8\n",
      "297/297 [==============================] - 4s 13ms/step - loss: 0.0492 - accuracy: 0.9831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fac800a5070>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running LSTM Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Activation, Dropout, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word2idx.keys()),output_dim=20,input_length=25))\n",
    "model.add(TimeDistributed(Dense(len(label2idx.keys()))))\n",
    "model.add(Bidirectional(LSTM(units=50,return_sequences=True), merge_mode = 'concat'))\n",
    "model.add(Dense(len(label2idx.keys()), activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Reference Tensor Flow documentation \n",
    "\n",
    "# Fit model\n",
    "model.fit(x, np.array(y), batch_size=16, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "d0c09b1e-2fcd-4b80-ab3e-e0b3c2f2a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "#y_validation = np.argmax(y_validation, -1)\n",
    "y_pred = [[idx2label[i] for i in row] for row in y_pred]\n",
    "#y_validation = [[idx2label[i] for i in row] for row in y_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "df77c959-aa55-4162-ac91-57b92991ba14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1259 1259\n"
     ]
    }
   ],
   "source": [
    "# Checking if lengths of predictions and row ids match\n",
    "\n",
    "print(len(y_pred), len(row_id_text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "58dda683-0ebf-40cf-b5d2-e9f899d7b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing BIO labels to string for each sentence\n",
    "\n",
    "y_pred_temp = []\n",
    "\n",
    "for sub_list in y_pred:\n",
    "    sub_list = ' '.join([label for label in sub_list])\n",
    "    y_pred_temp.append(sub_list)\n",
    "\n",
    "y_pred = y_pred_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "3722190e-5c23-4730-b47a-b74c456339e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting label results in LABELSEQ.txt format\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "column_names = ['ID','TAGSEQ']\n",
    "test_labels_df = pd.DataFrame(zip(row_id_text_test,y_pred), columns=column_names)\n",
    "\n",
    "test_labels_df.to_csv('TEST_LABELSEQ.txt',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f34f08-f959-48a1-87fe-9542d229bd92",
   "metadata": {},
   "source": [
    "## **Running CRF on Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "63771425-a39d-4cc8-9aed-85fd2e1aed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(f):\n",
    "    data = open(f,'r').readlines()[1:]\n",
    "    row_id = [i.split('\\t')[0].strip() for i in data]\n",
    "    data = [i.split('\\t')[1].strip().split(' ') for i in data]\n",
    "    return row_id,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "118fe4cc-c4ed-4507-97cb-a4fb6ba602bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id_text, texts = read_file('REVIEW_TEXT.txt')\n",
    "row_id_tags, tags = read_file('REVIEW_LABELSEQ.txt')\n",
    "row_id_text_test, texts_test = read_file('TEST_REVIEW_TEXT.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "413cce61-22a4-46e4-a918-390971583bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def word2features(word):\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.upper()': word.upper(),\n",
    "        'word[:1]': word[:1],\n",
    "        'word[:2]': word[:2],\n",
    "        'word[:3]': word[:3],\n",
    "        'word[:4]': word[:4],\n",
    "        'word[-1:]': word[-1:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-4:]': word[-4:],\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.ispunctuation()': (word in string.punctuation),\n",
    "        'len(word)': len(word)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def text2features(text):\n",
    "    return [word2features(i) for i in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "eef158db-45fd-45f3-ab5a-266acfff35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [text2features(text) for text in texts]\n",
    "y_train = tags\n",
    "x_test = [text2features(text) for text in texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a562c632-10aa-4055-924f-a8894338a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "crf = CRF(algorithm = 'ap')\n",
    "\n",
    "# Training and Prediction\n",
    "crf.fit(x_train, y_train)\n",
    "y_pred = crf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "07f57802-24ce-4533-9b0d-b926fa9e727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1259 1259\n"
     ]
    }
   ],
   "source": [
    "# Checking if lengths of predictions and row ids match\n",
    "\n",
    "print(len(y_pred), len(row_id_text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c9ae9bda-2fc2-47e0-ad37-6d39b4aaea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing BIO labels to string for each sentence\n",
    "\n",
    "y_pred_temp = []\n",
    "\n",
    "for sub_list in y_pred:\n",
    "    sub_list = ' '.join([label for label in sub_list])\n",
    "    y_pred_temp.append(sub_list)\n",
    "\n",
    "y_pred = y_pred_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "67792508-7132-4719-8258-28197b98c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting label results in LABELSEQ.txt format\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "column_names = ['ID','TAGSEQ']\n",
    "test_labels_df = pd.DataFrame(zip(row_id_text_test,y_pred), columns=column_names)\n",
    "\n",
    "test_labels_df.to_csv('TEST_LABELSEQ_CRF.txt',sep='\\t',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
